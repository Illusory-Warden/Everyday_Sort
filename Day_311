# üîπ State-Space Representation (SSR)

**Idea in one line:**
It‚Äôs a way to describe a problem as **all the possible situations (states)** you can be in, and **all the possible moves (actions)** that take you from one state to another‚Äîso a computer can **search** for a solution.

Think of it like a **map**: circles = states, arrows = actions. Once we have this map, we can run search methods (BFS/DFS/A\*/Minimax) to find a path to the goal.

---

## üß© The pieces of a state-space (beginner friendly)

* **State:** A snapshot of ‚Äúwhere you are‚Äù in the problem.
* **Initial state:** Where you start.
* **Actions / Operators:** What you‚Äôre allowed to do from a state.
* **Transition model:** If you take an action, **what new state** do you reach?
* **Goal test:** How you recognize ‚Äúwe‚Äôre done.‚Äù
* **Path cost / Utility (optional):** How good/cheap a path or outcome is (used for optimality).

> Your course explicitly calls out ‚Äústate space representation‚Äù as a core concept and outcome to master.&#x20;

---

## üß† Why SSR matters

* Converts any problem into a **searchable structure** ‚Üí we can apply general algorithms.
* Makes reasoning **systematic** and **repeatable**.
* Helps spot issues like **combinatorial explosion** early (too many states).

---

## üéÆ Example: Tic-Tac-Toe as a state space

* **State:** 3√ó3 board contents + whose turn it is (`X`, `O`, or empty `-`).
* **Initial state:** All cells empty; usually `X` to move.
* **Actions:** Place your mark in any empty cell.
* **Transition:** Fill that cell and switch the player.
* **Goal test:** Someone has three in a row, or the board is full (draw).
* **Evaluation (for game AI):** At end states, assign scores (e.g., +10 win, ‚àí10 loss, 0 draw) and use **Minimax** to choose moves.&#x20;

Tiny text form of a state (just to visualize):

```
X O -
- X -
O - -
Turn: X
```

Your PDF also lists the **rules** and the **Minimax idea** for this game, which is exactly how we use the state space to make decisions.&#x20;

---

## üå≥ State-space graph vs. search tree

| Concept               | What it shows                                                                                                        | When used                                                         |
| --------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **State-space graph** | Every **unique state** is a node; edges are actions. Can have cycles/shared nodes.                                   | The true structure of the problem.                                |
| **Search tree**       | Your **exploration history** from the start state; nodes can repeat the same underlying state in different branches. | What algorithms (BFS/DFS/Minimax) actually build while searching. |

---

## üß≠ Another quick example (non-game)

**Route finding (maps):**

* State = current city; Actions = drive to neighboring city; Goal test = reach target city; Path cost = total distance/time. (Same SSR idea, different domain.)

---

## ‚ö†Ô∏è Common pitfalls & how we cope

* **State explosion:** Too many states.

  * Fix: heuristics (A\*), pruning (Œ±-Œ≤ in games), exploiting symmetry, abstraction.
* **Redundant revisits / cycles:**

  * Fix: ‚ÄúVisited‚Äù sets, canonical representations (e.g., sorted forms).
* **Poor state design:**

  * Fix: include exactly what‚Äôs needed (no more, no less) to decide the next move.

---

## üìù Quick checklist (to model any problem)

1. What is a **state** here?
2. How do I **start**?
3. What **actions** are legal?
4. After an action, what **state** do I land in?
5. How do I **recognize a goal**?
6. Do I care about **optimality** (path cost/utility)?
